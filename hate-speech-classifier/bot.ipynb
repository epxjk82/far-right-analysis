{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div>\n",
    "<center><h1>Twitter Response Bot</h1><br><img src=\"http://cdn.ultraswank.net/uploads/Monty_Python_and_the_Holy_Grail_2-1000x601.jpg\" width=600></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from markovbot import MarkovBot\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Load the Model Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('model/hate-speech-classifier.pkl')\n",
    "vec = joblib.load('model/hate-speech-vector.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PorterTokenizer(object):\n",
    "    \"\"\"Custom PorterTokenizer for TfidfVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc.translate(translate_table))]\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \"\"\"Vecotizer wrapper for sklearn TfidfVectorizer.\n",
    "\n",
    "    Allows passing of custom tokenizer\n",
    "\n",
    "    TODO: add more custom tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer=None,\n",
    "                 encoding='utf-8',\n",
    "                 stop_words='english',\n",
    "                 min_df=1,\n",
    "                 ngram_range=None):\n",
    "        self.tokenizers = {'porter': PorterTokenizer()}\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self.tokenizers[tokenizer],\n",
    "                                          encoding=encoding,\n",
    "                                          stop_words=stop_words,\n",
    "                                          min_df=min_df,\n",
    "                                          ngram_range=ngram_range)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.vectorizer.fit_transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(x)\n",
    "    \n",
    "def test_model(base_model, param_grid):\n",
    "    grid_clf = GridSearchCV(base_model, param_grid, cv=5)\n",
    "    grid_clf.fit(train_features, y_train)\n",
    "    preds = grid_clf.predict(test_features)\n",
    "    print(classification_report(y_test, preds))\n",
    "    return grid_clf\n",
    "\n",
    "def top_words(clf, label, top):\n",
    "    data = []\n",
    "    for i in clf.best_estimator_.coef_[label, :].argsort()[::-1][:top]:\n",
    "        top_words = (i, clf.best_estimator_.coef_[0, i], vec.vectorizer.get_feature_names()[i])\n",
    "        data.append(\"{}\".format(top_words[2]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'C': [1, 10, 100, 1000]}], pre_dispatch='2*n_jobs',\n",
      "       refit=True, return_train_score=True, scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hate_words = top_words(clf, 0, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Bot Code</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot says: We do routines and chorus scenes With footwork impeccable. We dine well here in Camelot.\n"
     ]
    }
   ],
   "source": [
    "bot = MarkovBot()\n",
    "# read input of text replies and build random conversational sentences\n",
    "bot.read('replies.txt')\n",
    "text = bot.generate_text(15, seedword=[''])\n",
    "print('bot says: '+text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Twitter API stuff\n",
    "cons_key = 'fai4h6SN6gFM9nhQGu2A72mHY'\n",
    "cons_secret = 'GQLWRbIMeVcT6BT1uLP79oA2JmS7WUyZUgN7KoHZGPG4v7KOPi'\n",
    "access_token = '848222105535602688-doZSwW16KKC1PQd4YKenIuYUTe6n4nh'\n",
    "access_token_secret = 'MNI1HQDUIIJ01AKkjyyopSdz7QNlhVG3Ryox1Fpv3TPi2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Log in to Twitter\n",
    "bot.twitter_login(cons_key, cons_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prefix = None\n",
    "suffix = ''\n",
    "maxconvdepth = 2\n",
    "bot.twitter_autoreply_start(targetstring = '', keywords=hate_words, prefix=prefix, suffix=suffix, maxconvdepth=maxconvdepth)\n",
    "bot.twitter_tweeting_start(days=0, hours=19, minutes=30, keywords=None, prefix=None, suffix='#BleepBloop')\n",
    "secsinweek = 60\n",
    "time.sleep(secsinweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweetbot.twitter_autoreply_stop()\n",
    "tweetbot.twitter_tweeting_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
